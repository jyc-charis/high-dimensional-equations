{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep2BSDE solver with hard-coded Black-Scholes-Barenblatt euqation.\n",
    "\"\"\"\n",
    "\n",
    "import time, datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.training import moving_averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"BSB\"\n",
    "d = 100\n",
    "batch_size = 64\n",
    "T = 1.0\n",
    "N = 20\n",
    "h = T / N\n",
    "sqrth = np.sqrt(h)\n",
    "n_maxstep = 500\n",
    "n_displaystep = 100\n",
    "n_neuronForA = [d, d, d, d]\n",
    "n_neuronForGamma = [d, d, d, d ** 2]\n",
    "Xinit = np.array([1.0, 0.5] * 50)\n",
    "mu = 0\n",
    "sigma = 0.4\n",
    "sigma_min = 0.1\n",
    "sigma_max = 0.4\n",
    "r = 0.05\n",
    "\n",
    "_extra_train_ops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_value(W):\n",
    "    # tf.cast: Casts a tensor to a new type.\n",
    "    return sigma_max *  \\\n",
    "        tf.cast(tf.math.greater_equal(W, tf.cast(0, tf.float64)), \n",
    "                tf.float64) + \\\n",
    "        sigma_min * tf.cast(tf.greater(tf.cast(0, tf.float64), W), \n",
    "                            tf.float64) \n",
    "# tf.math.greater_equal: Returns the truth value of (x >= y) element-wise.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_tf(t, X, Y ,Z, Gamma):\n",
    "    # tf.expand_dims: Returns a tensor with a length 1 axis inserted at index axis\n",
    "    # tf.linalg.trace: Compute the trace of a tensor x.\n",
    "    return -0.5 * tf.compat.v1.expand_dims(tf.linalg.trace(tf.square(tf.expand_dims(X, -1)) * \\\n",
    "                                         (tf.square(sigma_value(Gamma)) - sigma ** 2) * Gamma), -1) + \\\n",
    "            r * (Y - tf.reduce_sum(X * Z, 1, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_tf(X):\n",
    "    return tf.reduce_sum(tf.square(X), 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_function(X):\n",
    "    return sigma * tf.linalg.diag(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_function(X):\n",
    "    return mu * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_time_net(x, name, isgamma = False):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        x_norm = _batch_norm(x, name = 'layer0_normalization')\n",
    "        layer1 = _one_layer(x_norm, (1 - isgamma) * n_neuronForA[1] \\\n",
    "                           + isgamma * n_neuronForGamma[1], name = 'layer1')\n",
    "        layer2 = _one_layer(layer1, (1 - isgamma) * n_neuronForA[2] \\\n",
    "                           + isgamma * n_neuronForGamma[2], name = 'layer2')\n",
    "        z = _one_layer(layer2, (1 - isgamma) * n_neuronForA[3] \\\n",
    "                           + isgamma * n_neuronForGamma[3], activation_fn = None, \n",
    "                      name = 'final')\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_layer(input_, output_size, activation_fn = tf.nn.relu, \n",
    "               stddev = 5.0, name = 'linear'): \n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        shape = input_.get_shape().as_list()\n",
    "        w = tf.compat.v1.get_variable('Matrix', [shape[1], output_size], \n",
    "                            tf.float64, \n",
    "                            tf.random_normal_initializer(\n",
    "                            stddev = stddev / np.sqrt(shape[1] + output_size)))\n",
    "        hidden = tf.matmul(input_, w) \n",
    "        hidden_bn = _batch_norm(hidden, name = 'normalization')\n",
    "        if activation_fn:\n",
    "            return activation_fn(hidden_bn)\n",
    "        else:\n",
    "            return hidden_bn\n",
    "\n",
    "def _batch_norm(x, name):\n",
    "    \"\"\"\n",
    "    Batch normalization\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        params_shape = [x.get_shape()[-1]]\n",
    "        beta = tf.compat.v1.get_variable(\"beta\", params_shape, tf.float64,\n",
    "                                        initializer = tf.random_normal_initializer(\n",
    "                                        0.0, stddev = 0.1))\n",
    "        gamma = tf.compat.v1.get_variable(\"gamma\", params_shape, tf.float64,\n",
    "                                        initializer = tf.random_normal_initializer(\n",
    "                                        0.1, 0.5))\n",
    "        moving_mean = tf.compat.v1.get_variable(\"moving_mean\", params_shape, tf.float64,\n",
    "                                        initializer = tf.constant_initializer(\n",
    "                                        0.0), trainable = False)\n",
    "        moving_variance = tf.compat.v1.get_variable(\"moving_variance\",  params_shape, tf.float64,\n",
    "                                        initializer = tf.constant_initializer(\n",
    "                                        1.0), trainable = False)\n",
    "        # tf.nn.moments: Calculates the mean and variance of x\n",
    "        mean, variance = tf.nn.moments(x, [0], name = \"moments\")\n",
    "        _extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "        moving_mean, mean, 0.99))\n",
    "        _extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "        moving_variance, variance, 0.99))\n",
    "        y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
    "        y.set_shape(x.get_shape())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ycjia\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "step:  0 loss:  5653.313355496821 Y0:  [0.87383026] learning rate:  1.0\n",
      "step:  100 loss:  401.07156462349235 Y0:  [57.56265741] learning rate:  1.0\n",
      "step:  200 loss:  176.2486533025562 Y0:  [75.08536845] learning rate:  0.5\n",
      "step:  300 loss:  114.59785721053751 Y0:  [76.44565893] learning rate:  0.5\n",
      "step:  400 loss:  80.12551460035486 Y0:  [77.08025597] learning rate:  0.25\n",
      "step:  500 loss:  41.03406366168464 Y0:  [77.15279633] learning rate:  0.25\n",
      "running time: 527.9619085788727\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess: # session: a class for running TensorFlow operations.\n",
    "    # background dynamics\n",
    "    dW = tf.random.normal(shape = [batch_size, d], stddev = 1,\n",
    "                         dtype = tf.float64)\n",
    "    \n",
    "    # initial values of the stochastic processes\n",
    "    X = tf.Variable(np.ones([batch_size, d]) * Xinit, \n",
    "                    dtype = tf.float64,\n",
    "                    trainable = False) # why trainable is False?\n",
    "    # When building a machine learning model it is often convenient to distinguish between variables \n",
    "    # holding trainable model parameters and other variables such as a step variable used to count training steps\n",
    "    Y0 = tf.Variable(tf.random.uniform([1],\n",
    "                     minval = 0, maxval = 1,\n",
    "                     dtype = tf.float64),\n",
    "                     name = 'Y0')\n",
    "    Z0 = tf.Variable(tf.random.uniform([1,d],\n",
    "                     minval = -.1, maxval = .1, \n",
    "                     dtype = tf.float64),\n",
    "                     name = 'Z0')\n",
    "    Gamma0 = tf.Variable(tf.random.uniform([d, d],\n",
    "                         minval = -1, maxval = 1,\n",
    "                         dtype = tf.float64),\n",
    "                         name = 'Gamma0')\n",
    "    A0 = tf.Variable(tf.random.uniform([1, d],\n",
    "                     minval = -.1, maxval = .1,\n",
    "                     dtype = tf.float64),\n",
    "                     name = 'A0')\n",
    "    allones = tf.ones(shape = [batch_size, 1],\n",
    "                      dtype = tf.float64,\n",
    "                      name = 'MatrixOfOnes')\n",
    "    Y = allones * Y0\n",
    "    Z = tf.matmul(allones, Z0)\n",
    "    A = tf.matmul(allones, A0)\n",
    "    Gamma = tf.multiply(tf.ones(shape = [batch_size, d, d],\n",
    "                        dtype = tf.float64), Gamma0)\n",
    "    \n",
    "    # forward discretization \n",
    "    with tf.compat.v1.variable_scope('forward'):\n",
    "        for t in range(N - 1):\n",
    "            # Y update inside the loop\n",
    "            dX = mu * X * h + sqrth * sigma * X * dW\n",
    "            Y = Y + f_tf(t * h, X, Y, Z, Gamma) * h \\\n",
    "            + tf.reduce_sum(Z * dX, 1, keepdims = True)\n",
    "            X = X + dX\n",
    "            \n",
    "            Z = Z + A * h\\\n",
    "            + tf.squeeze(tf.matmul(Gamma,\n",
    "                         tf.compat.v1.expand_dims(dX, -1),\n",
    "                         transpose_b = False)) # tf.squeeze: removes dimensions of size 1 from the shape of a tensor.\n",
    "            A = _one_time_net(X, str(t + 1) + \"A\") / d\n",
    "            Gamma =  _one_time_net(X, str(t+1) + \"Gamma\", \n",
    "                                  isgamma = True) / d ** 2\n",
    "            Gamma = tf.reshape(Gamma, [batch_size, d, d])\n",
    "            dW = tf.random.normal(shape = [batch_size, d],\n",
    "                                  stddev = 1, dtype = tf.float64)\n",
    "            \n",
    "        dX = mu * X * h + sqrth * sigma * X * dW\n",
    "        Y = Y + f_tf((N - 1) * h, X, Y, Z, Gamma) * h \\\n",
    "            + tf.reduce_sum(Z * dX, 1, keepdims = True)\n",
    "        X = X + dX\n",
    "           \n",
    "        loss = tf.reduce_mean(tf.square(Y - g_tf(X)))\n",
    "        \n",
    "    # specifying the optimizer\n",
    "    global_step = tf.compat.v1.get_variable('global_step', [],\n",
    "                                  initializer = tf.constant_initializer(0),\n",
    "                                  trainable = False, dtype = tf.int32)\n",
    "    # tf.train.exponential_decay: applies exponential decay to the learning rate.\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(1.0, global_step,\n",
    "                                               decay_steps = 200, decay_rate = 0.5, staircase = True)\n",
    "    # tf.trainable_variables: returns all variables created with trainable=True\n",
    "    trainable_variables = tf.compat.v1.trainable_variables()\n",
    "    grads = tf.gradients(loss, trainable_variables)\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    apply_op = optimizer.apply_gradients(zip(grads, trainable_variables),\n",
    "        global_step = global_step, name = 'train_step')\n",
    "    train_ops = [apply_op] + _extra_train_ops\n",
    "    train_op = tf.group(*train_ops)\n",
    "    \n",
    "    with tf.control_dependencies([train_op]):\n",
    "        train_op_2 = tf.identity(loss, name = 'train_op2')\n",
    "        \n",
    "    # to save history\n",
    "    learning_rates = []\n",
    "    y0_values = []\n",
    "    losses = []\n",
    "    running_time = []\n",
    "    steps = []\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    try:\n",
    "        # the actual training loop\n",
    "        for _ in range(n_maxstep + 1):\n",
    "            y0_value, step = sess.run([Y0, global_step])\n",
    "            currentLoss, currentLearningRate = sess.run(\n",
    "            [train_op_2, learning_rate])\n",
    "            \n",
    "            steps.append(step)\n",
    "            losses.append(currentLoss)\n",
    "            y0_values.append(y0_value)\n",
    "            learning_rates.append(currentLearningRate)\n",
    "            running_time.append(time.time() - start_time)\n",
    "            \n",
    "            \n",
    "            if step % n_displaystep == 0:\n",
    "                print(\"step: \", step,\n",
    "                      \"loss: \", currentLoss,\n",
    "                      \"Y0: \", y0_value,\n",
    "                      \"learning rate: \", currentLearningRate)\n",
    "        end_time = time.time()\n",
    "        print(\"running time:\", end_time - start_time)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nmanually disengageed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((len(y0_values),5))\n",
    "output[:, 0] = steps\n",
    "output[:, 1] = losses\n",
    "output[:, 2] = y0_values\n",
    "output[:, 3] = learning_rates\n",
    "output[:, 4] = running_time\n",
    "\n",
    "np.savetxt(str(name) + \"_d\" + str(d) + \"_\" + \\\n",
    "          datetime.datetime.now().strftime('%Y-%m-%d-%H %M %S') + \".csv\",\n",
    "          output,\n",
    "          delimiter = \",\",\n",
    "          header = \"step, loss function, Y0, learning rate, running time\",\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
